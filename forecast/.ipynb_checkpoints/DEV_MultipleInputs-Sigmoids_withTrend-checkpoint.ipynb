{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import *\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "import stldecompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>QQQ.Open</th>\n",
       "      <th>QQQ.High</th>\n",
       "      <th>QQQ.Low</th>\n",
       "      <th>QQQ.Close</th>\n",
       "      <th>QQQ.Volume</th>\n",
       "      <th>QQQ.Adjusted</th>\n",
       "      <th>GSPC.Open</th>\n",
       "      <th>GSPC.High</th>\n",
       "      <th>GSPC.Low</th>\n",
       "      <th>...</th>\n",
       "      <th>XLU.Low</th>\n",
       "      <th>XLU.Close</th>\n",
       "      <th>XLU.Volume</th>\n",
       "      <th>XLU.Adjusted</th>\n",
       "      <th>XLRE.Open</th>\n",
       "      <th>XLRE.High</th>\n",
       "      <th>XLRE.Low</th>\n",
       "      <th>XLRE.Close</th>\n",
       "      <th>XLRE.Volume</th>\n",
       "      <th>XLRE.Adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>187.050003</td>\n",
       "      <td>190.199997</td>\n",
       "      <td>186.759995</td>\n",
       "      <td>189.940002</td>\n",
       "      <td>38371800</td>\n",
       "      <td>189.940002</td>\n",
       "      <td>2908.889893</td>\n",
       "      <td>2937.320068</td>\n",
       "      <td>2898.209961</td>\n",
       "      <td>...</td>\n",
       "      <td>57.860001</td>\n",
       "      <td>57.980000</td>\n",
       "      <td>13389500.0</td>\n",
       "      <td>57.980000</td>\n",
       "      <td>36.189999</td>\n",
       "      <td>36.340000</td>\n",
       "      <td>35.970001</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>3377200</td>\n",
       "      <td>36.200001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>2019-05-07</td>\n",
       "      <td>187.869995</td>\n",
       "      <td>188.669998</td>\n",
       "      <td>184.509995</td>\n",
       "      <td>186.240005</td>\n",
       "      <td>58818100</td>\n",
       "      <td>186.240005</td>\n",
       "      <td>2913.030029</td>\n",
       "      <td>2913.030029</td>\n",
       "      <td>2862.600098</td>\n",
       "      <td>...</td>\n",
       "      <td>57.529999</td>\n",
       "      <td>57.799999</td>\n",
       "      <td>12542200.0</td>\n",
       "      <td>57.799999</td>\n",
       "      <td>36.080002</td>\n",
       "      <td>36.139999</td>\n",
       "      <td>35.320000</td>\n",
       "      <td>35.580002</td>\n",
       "      <td>3711800</td>\n",
       "      <td>35.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2019-05-08</td>\n",
       "      <td>185.630005</td>\n",
       "      <td>187.169998</td>\n",
       "      <td>184.960007</td>\n",
       "      <td>185.770004</td>\n",
       "      <td>37165300</td>\n",
       "      <td>185.770004</td>\n",
       "      <td>2879.610107</td>\n",
       "      <td>2897.959961</td>\n",
       "      <td>2873.280029</td>\n",
       "      <td>...</td>\n",
       "      <td>56.959999</td>\n",
       "      <td>57.009998</td>\n",
       "      <td>28692400.0</td>\n",
       "      <td>57.009998</td>\n",
       "      <td>35.650002</td>\n",
       "      <td>35.919998</td>\n",
       "      <td>35.564999</td>\n",
       "      <td>35.580002</td>\n",
       "      <td>3831500</td>\n",
       "      <td>35.580002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3108</th>\n",
       "      <td>2019-05-09</td>\n",
       "      <td>183.699997</td>\n",
       "      <td>185.410004</td>\n",
       "      <td>182.089996</td>\n",
       "      <td>184.770004</td>\n",
       "      <td>55351700</td>\n",
       "      <td>184.770004</td>\n",
       "      <td>2859.840088</td>\n",
       "      <td>2875.969971</td>\n",
       "      <td>2836.399902</td>\n",
       "      <td>...</td>\n",
       "      <td>56.660000</td>\n",
       "      <td>56.939999</td>\n",
       "      <td>16173800.0</td>\n",
       "      <td>56.939999</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>35.740002</td>\n",
       "      <td>35.259998</td>\n",
       "      <td>35.700001</td>\n",
       "      <td>5684500</td>\n",
       "      <td>35.700001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>2019-05-10</td>\n",
       "      <td>184.110001</td>\n",
       "      <td>185.889999</td>\n",
       "      <td>181.029999</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>58497400</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>2863.100098</td>\n",
       "      <td>2891.310059</td>\n",
       "      <td>2825.389893</td>\n",
       "      <td>...</td>\n",
       "      <td>56.820000</td>\n",
       "      <td>57.959999</td>\n",
       "      <td>25157000.0</td>\n",
       "      <td>57.959999</td>\n",
       "      <td>35.619999</td>\n",
       "      <td>36.200001</td>\n",
       "      <td>35.595001</td>\n",
       "      <td>36.110001</td>\n",
       "      <td>3268400</td>\n",
       "      <td>36.110001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Index    QQQ.Open    QQQ.High     QQQ.Low   QQQ.Close  QQQ.Volume  \\\n",
       "3105  2019-05-06  187.050003  190.199997  186.759995  189.940002    38371800   \n",
       "3106  2019-05-07  187.869995  188.669998  184.509995  186.240005    58818100   \n",
       "3107  2019-05-08  185.630005  187.169998  184.960007  185.770004    37165300   \n",
       "3108  2019-05-09  183.699997  185.410004  182.089996  184.770004    55351700   \n",
       "3109  2019-05-10  184.110001  185.889999  181.029999  185.000000    58497400   \n",
       "\n",
       "      QQQ.Adjusted    GSPC.Open    GSPC.High     GSPC.Low      ...        \\\n",
       "3105    189.940002  2908.889893  2937.320068  2898.209961      ...         \n",
       "3106    186.240005  2913.030029  2913.030029  2862.600098      ...         \n",
       "3107    185.770004  2879.610107  2897.959961  2873.280029      ...         \n",
       "3108    184.770004  2859.840088  2875.969971  2836.399902      ...         \n",
       "3109    185.000000  2863.100098  2891.310059  2825.389893      ...         \n",
       "\n",
       "        XLU.Low  XLU.Close  XLU.Volume  XLU.Adjusted  XLRE.Open  XLRE.High  \\\n",
       "3105  57.860001  57.980000  13389500.0     57.980000  36.189999  36.340000   \n",
       "3106  57.529999  57.799999  12542200.0     57.799999  36.080002  36.139999   \n",
       "3107  56.959999  57.009998  28692400.0     57.009998  35.650002  35.919998   \n",
       "3108  56.660000  56.939999  16173800.0     56.939999  35.500000  35.740002   \n",
       "3109  56.820000  57.959999  25157000.0     57.959999  35.619999  36.200001   \n",
       "\n",
       "       XLRE.Low  XLRE.Close  XLRE.Volume  XLRE.Adjusted  \n",
       "3105  35.970001   36.200001      3377200      36.200001  \n",
       "3106  35.320000   35.580002      3711800      35.580002  \n",
       "3107  35.564999   35.580002      3831500      35.580002  \n",
       "3108  35.259998   35.700001      5684500      35.700001  \n",
       "3109  35.595001   36.110001      3268400      36.110001  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig = pd.read_csv('../data/files/multiple_concatenated_tickers.csv')\n",
    "data_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import *\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "def df_from_fred(setname):\n",
    "    # Make GET Request\n",
    "    response = urlopen(url_for(setname))\n",
    "    # Read response data\n",
    "    data = response.read()\n",
    "    # Convert binary text to utf-8\n",
    "    text = data.decode('utf-8')\n",
    "    # Convert text file to pandas dataframe\n",
    "    TEXTDATA = StringIO(text)\n",
    "    df = pd.read_csv(TEXTDATA, sep=\",\")\n",
    "    return df\n",
    "\n",
    "def url_for(series):\n",
    "    \"\"\"function takes FRED series name as input. For example, GDPC1, or HOUST.\"\"\"\n",
    "    return \"https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=968&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=\"+series+\"&scale=left&cosd=1947-01-01&coed=2019-01-01&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Quarterly&fam=avg&fgst=lin&fgsnd=2009-06-01&line_index=1&transformation=lin&vintage_date=2019-05-07&revision_date=2019-05-07&nd=1947-01-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GDPC1 = df_from_fred(\"GDPC1\")\n",
    "time.sleep(5)\n",
    "M2 = df_from_fred(\"M2\")\n",
    "time.sleep(5)\n",
    "CPALTT01USQ657N = df_from_fred(\"CPALTT01USQ657N\")\n",
    "time.sleep(5)\n",
    "PPIACO = df_from_fred(\"PPIACO\")\n",
    "time.sleep(5)\n",
    "UMCSENT = df_from_fred(\"UMCSENT\")\n",
    "time.sleep(5)\n",
    "PAYEMS = df_from_fred(\"PAYEMS\")\n",
    "time.sleep(5)\n",
    "RRSFS = df_from_fred(\"RRSFS\")\n",
    "time.sleep(5)\n",
    "HOUST = df_from_fred(\"HOUST\")\n",
    "time.sleep(5)\n",
    "ISRATIO = df_from_fred(\"ISRATIO\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Others\n",
    "IPMAN = df_from_fred(\"IPMAN\")\n",
    "time.sleep(5)\n",
    "MPU9900063 = df_from_fred(\"MPU9900063\")\n",
    "time.sleep(5)\n",
    "PCU33443344 = df_from_fred(\"PCU33443344\")\n",
    "time.sleep(5)\n",
    "MEHOINUSA672N = df_from_fred(\"MEHOINUSA672N\")\n",
    "time.sleep(5)\n",
    "TCMDO = df_from_fred(\"TCMDO\")\n",
    "time.sleep(5)\n",
    "FGTCMDODNS = df_from_fred(\"FGTCMDODNS\")\n",
    "time.sleep(5)\n",
    "ADSLFAA027N = df_from_fred(\"ADSLFAA027N\")\n",
    "time.sleep(5)\n",
    "NCBCMDPMVCE = df_from_fred(\"NCBCMDPMVCE\")\n",
    "time.sleep(5)\n",
    "FGCCSAQ027S = df_from_fred(\"FGCCSAQ027S\")\n",
    "time.sleep(5)\n",
    "ASTNITA = df_from_fred(\"ASTNITA\")\n",
    "time.sleep(5)\n",
    "PCETRIM12M159SFRBDAL = df_from_fred(\"PCETRIM12M159SFRBDAL\")\n",
    "\n",
    "# IPMAN, MPU9900063, PCU33443344, MEHOINUSA672N, TCMDO, FGTCMDODNS, ADSLFAA027N, NCBCMDPMVCE, FGCCSAQ027S, ASTNITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data_orig\\\n",
    "  .merge(GDPC1, how=\"left\", left_on=data_orig.Index, right_on=GDPC1.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(M2, how=\"left\", left_on=data_orig.Index, right_on=M2.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(CPALTT01USQ657N, how=\"left\", left_on=data_orig.Index, right_on=CPALTT01USQ657N.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(PPIACO, how=\"left\", left_on=data_orig.Index, right_on=PPIACO.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(UMCSENT, how=\"left\", left_on=data_orig.Index, right_on=UMCSENT.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(PAYEMS, how=\"left\", left_on=data_orig.Index, right_on=PAYEMS.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(RRSFS, how=\"left\", left_on=data_orig.Index, right_on=RRSFS.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(HOUST, how=\"left\", left_on=data_orig.Index, right_on=HOUST.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(ISRATIO, how=\"left\", left_on=data_orig.Index, right_on=ISRATIO.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(PCETRIM12M159SFRBDAL, how=\"left\", left_on=data_orig.Index, right_on=PCETRIM12M159SFRBDAL.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(ASTNITA, how=\"left\", left_on=data_orig.Index, right_on=ASTNITA.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(FGCCSAQ027S, how=\"left\", left_on=data_orig.Index, right_on=FGCCSAQ027S.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(NCBCMDPMVCE, how=\"left\", left_on=data_orig.Index, right_on=NCBCMDPMVCE.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(ADSLFAA027N, how=\"left\", left_on=data_orig.Index, right_on=ADSLFAA027N.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(FGTCMDODNS, how=\"left\", left_on=data_orig.Index, right_on=FGTCMDODNS.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(TCMDO, how=\"left\", left_on=data_orig.Index, right_on=TCMDO.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(MEHOINUSA672N, how=\"left\", left_on=data_orig.Index, right_on=MEHOINUSA672N.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(PCU33443344, how=\"left\", left_on=data_orig.Index, right_on=PCU33443344.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(MPU9900063, how=\"left\", left_on=data_orig.Index, right_on=MPU9900063.DATE).fillna(method=\"ffill\")\\\n",
    "  .drop(\"key_0\", axis=1)\\\n",
    "  .merge(IPMAN, how=\"left\", left_on=data_orig.Index, right_on=IPMAN.DATE).fillna(method=\"ffill\")\\\n",
    "  .fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_orig = data_orig.drop([i for i in data_orig.columns if \"_\" in i]+[\"DATE\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_lookup = dict([(i[1].split('.')[0], int(i[0])) for i in enumerate(list(data_orig.columns)) if 'Open' in i[1]])\n",
    "ticker_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CHOSENTICKER = 'QQQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_lookback = 1\n",
    "theta = 0.05 # This needs to go between pct_change and argmax apply\n",
    "pct_df = data_orig.iloc[:,list(ticker_lookup.values())].pct_change(days_lookback).apply(lambda y: y.argmax(), axis=1).fillna(\"QQQ.Open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v+'.Open': k for k, v in dict(enumerate(list(ticker_lookup.keys()))).items()}\n",
    "pct_df.map(lambda x: inv_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-trend data\n",
    "# data_orig_detrended = signal.detrend(data_orig.iloc[:,1:])\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# scaler = scaler.fit(data_orig.iloc[:,1:])\n",
    "# data_mat = scaler.transform(data_orig.iloc[:,1:])\n",
    "data_mat = scaler.fit_transform(data_orig.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 365 # days to use for prediction\n",
    "data = np.array((data_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = seq_len + 1\n",
    "result = []\n",
    "for index in range(len(data) - sequence_length):\n",
    "    result.append(data[index: index + sequence_length])\n",
    "\n",
    "result = np.array(result)\n",
    "\n",
    "row = round(0.9 * result.shape[0])\n",
    "train = result[:int(row), :] # Create training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.shuffle(train)\n",
    "x_train = result[:int(row), :]\n",
    "y_train = to_categorical(pct_df.map(lambda x: inv_map[x]))[:int(row)]\n",
    "x_test = result[int(row):, :]\n",
    "y_test = to_categorical(pct_df.map(lambda x: inv_map[x]))[int(row):]\n",
    "\n",
    "[x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = 250\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_dim=data.shape[1],\n",
    "    output_dim=LAYERS,\n",
    "    return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(\n",
    "    LAYERS,\n",
    "    return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    output_dim=43))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "start = time.time()\n",
    "rmsprop = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n",
    "print('compilation time : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELNAME = 'multiplemodeltest_withTrend3'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('./models/'+MODELNAME+'_best.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATIONSIZE = 0.05\n",
    "EPOCHS = 500\n",
    "model = keras.models.load_model('./models/'+MODELNAME+'_best.hdf5') \n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    nb_epoch=EPOCHS,\n",
    "    validation_split=VALIDATIONSIZE,\n",
    "    callbacks = [reduce_lr_loss, earlyStopping, mcp_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = keras.models.load_model('./models/'+MODELNAME+'_best.hdf5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5 refers to days of data. 5 days worth. each 1 row has 36 features\n",
    "days = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dd = defaultdict(list)\n",
    "qq = defaultdict(list)\n",
    "for d in range(days):\n",
    "    li = []\n",
    "    for k in sorted(zip(list(ticker_lookup.keys()), best_model.predict(np.reshape(data[-days:], (days, 1, data.shape[1])))[d]), key = lambda x: x[1], reverse=True):\n",
    "        dd[k[0]].append(k[1])\n",
    "        li.append(k[0])\n",
    "    qq[d] = li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[qq[i][:10] for i in range(days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_map = {v:k for k, v in inv_map.items()}\n",
    "# model.predict(np.reshape(data[-days:], (days, 1, data.shape[1])))[0].argmax()\n",
    "[lookup_map[best_model.predict(np.reshape(data[-days:], (days, 1, data.shape[1])))[i].argmax()] for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([k for k in best_model.predict(np.reshape(data[-5:], (5, 1, data.shape[1])))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
