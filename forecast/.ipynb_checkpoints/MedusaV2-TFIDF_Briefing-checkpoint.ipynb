{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import *\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import signal\n",
    "import stldecompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>QQQ.Open</th>\n",
       "      <th>QQQ.High</th>\n",
       "      <th>QQQ.Low</th>\n",
       "      <th>QQQ.Close</th>\n",
       "      <th>QQQ.Volume</th>\n",
       "      <th>QQQ.Adjusted</th>\n",
       "      <th>TSLA.Open</th>\n",
       "      <th>TSLA.High</th>\n",
       "      <th>TSLA.Low</th>\n",
       "      <th>...</th>\n",
       "      <th>TLRY.Low</th>\n",
       "      <th>TLRY.Close</th>\n",
       "      <th>TLRY.Volume</th>\n",
       "      <th>TLRY.Adjusted</th>\n",
       "      <th>MU.Open</th>\n",
       "      <th>MU.High</th>\n",
       "      <th>MU.Low</th>\n",
       "      <th>MU.Close</th>\n",
       "      <th>MU.Volume</th>\n",
       "      <th>MU.Adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>2019-06-10</td>\n",
       "      <td>182.250000</td>\n",
       "      <td>184.850006</td>\n",
       "      <td>182.210007</td>\n",
       "      <td>183.149994</td>\n",
       "      <td>41385300</td>\n",
       "      <td>183.149994</td>\n",
       "      <td>210.250000</td>\n",
       "      <td>216.940002</td>\n",
       "      <td>209.009995</td>\n",
       "      <td>...</td>\n",
       "      <td>41.029999</td>\n",
       "      <td>43.139999</td>\n",
       "      <td>7322600</td>\n",
       "      <td>43.139999</td>\n",
       "      <td>34.599998</td>\n",
       "      <td>35.540001</td>\n",
       "      <td>34.520000</td>\n",
       "      <td>34.939999</td>\n",
       "      <td>21728000</td>\n",
       "      <td>34.939999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>2019-06-11</td>\n",
       "      <td>185.059998</td>\n",
       "      <td>185.399994</td>\n",
       "      <td>182.779999</td>\n",
       "      <td>183.399994</td>\n",
       "      <td>41260300</td>\n",
       "      <td>183.399994</td>\n",
       "      <td>219.139999</td>\n",
       "      <td>220.899994</td>\n",
       "      <td>213.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>40.139999</td>\n",
       "      <td>40.490002</td>\n",
       "      <td>2898900</td>\n",
       "      <td>40.490002</td>\n",
       "      <td>35.799999</td>\n",
       "      <td>35.990002</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>34.840000</td>\n",
       "      <td>19208600</td>\n",
       "      <td>34.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>182.899994</td>\n",
       "      <td>183.279999</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.339996</td>\n",
       "      <td>27758100</td>\n",
       "      <td>182.339996</td>\n",
       "      <td>222.949997</td>\n",
       "      <td>223.380005</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.430000</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>1702500</td>\n",
       "      <td>41.820000</td>\n",
       "      <td>34.009998</td>\n",
       "      <td>34.099998</td>\n",
       "      <td>32.730000</td>\n",
       "      <td>32.959999</td>\n",
       "      <td>28746500</td>\n",
       "      <td>32.959999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>183.100006</td>\n",
       "      <td>183.869995</td>\n",
       "      <td>182.740005</td>\n",
       "      <td>183.419998</td>\n",
       "      <td>23715800</td>\n",
       "      <td>183.419998</td>\n",
       "      <td>210.380005</td>\n",
       "      <td>214.899994</td>\n",
       "      <td>207.509995</td>\n",
       "      <td>...</td>\n",
       "      <td>40.400002</td>\n",
       "      <td>40.700001</td>\n",
       "      <td>1167200</td>\n",
       "      <td>40.700001</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>33.639999</td>\n",
       "      <td>33.009998</td>\n",
       "      <td>33.380001</td>\n",
       "      <td>16586700</td>\n",
       "      <td>33.380001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>182.479996</td>\n",
       "      <td>183.110001</td>\n",
       "      <td>181.940002</td>\n",
       "      <td>182.639999</td>\n",
       "      <td>22834000</td>\n",
       "      <td>182.639999</td>\n",
       "      <td>211.250000</td>\n",
       "      <td>216.649994</td>\n",
       "      <td>210.399994</td>\n",
       "      <td>...</td>\n",
       "      <td>38.700001</td>\n",
       "      <td>39.009998</td>\n",
       "      <td>1368200</td>\n",
       "      <td>39.009998</td>\n",
       "      <td>32.450001</td>\n",
       "      <td>32.840000</td>\n",
       "      <td>32.240002</td>\n",
       "      <td>32.660000</td>\n",
       "      <td>19700800</td>\n",
       "      <td>32.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 325 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Index    QQQ.Open    QQQ.High     QQQ.Low   QQQ.Close  QQQ.Volume  \\\n",
       "863  2019-06-10  182.250000  184.850006  182.210007  183.149994    41385300   \n",
       "864  2019-06-11  185.059998  185.399994  182.779999  183.399994    41260300   \n",
       "865  2019-06-12  182.899994  183.279999  182.000000  182.339996    27758100   \n",
       "866  2019-06-13  183.100006  183.869995  182.740005  183.419998    23715800   \n",
       "867  2019-06-14  182.479996  183.110001  181.940002  182.639999    22834000   \n",
       "\n",
       "     QQQ.Adjusted   TSLA.Open   TSLA.High    TSLA.Low     ...        TLRY.Low  \\\n",
       "863    183.149994  210.250000  216.940002  209.009995     ...       41.029999   \n",
       "864    183.399994  219.139999  220.899994  213.500000     ...       40.139999   \n",
       "865    182.339996  222.949997  223.380005  209.000000     ...       39.430000   \n",
       "866    183.419998  210.380005  214.899994  207.509995     ...       40.400002   \n",
       "867    182.639999  211.250000  216.649994  210.399994     ...       38.700001   \n",
       "\n",
       "     TLRY.Close  TLRY.Volume  TLRY.Adjusted    MU.Open    MU.High     MU.Low  \\\n",
       "863   43.139999      7322600      43.139999  34.599998  35.540001  34.520000   \n",
       "864   40.490002      2898900      40.490002  35.799999  35.990002  34.750000   \n",
       "865   41.820000      1702500      41.820000  34.009998  34.099998  32.730000   \n",
       "866   40.700001      1167200      40.700001  33.080002  33.639999  33.009998   \n",
       "867   39.009998      1368200      39.009998  32.450001  32.840000  32.240002   \n",
       "\n",
       "      MU.Close  MU.Volume  MU.Adjusted  \n",
       "863  34.939999   21728000    34.939999  \n",
       "864  34.840000   19208600    34.840000  \n",
       "865  32.959999   28746500    32.959999  \n",
       "866  33.380001   16586700    33.380001  \n",
       "867  32.660000   19700800    32.660000  \n",
       "\n",
       "[5 rows x 325 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_orig = pd.read_csv('../data/files/multiple_concatenated_tickers.csv')\n",
    "data_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>volatilityQQQ</th>\n",
       "      <th>volatilityTSLA</th>\n",
       "      <th>volatilityMSFT</th>\n",
       "      <th>volatilityINTC</th>\n",
       "      <th>volatilityAAPL</th>\n",
       "      <th>volatilityNFLX</th>\n",
       "      <th>volatilityAMZN</th>\n",
       "      <th>volatilityFB</th>\n",
       "      <th>volatilityGOOG</th>\n",
       "      <th>...</th>\n",
       "      <th>volatilityXLY</th>\n",
       "      <th>volatilityXLP</th>\n",
       "      <th>volatilityXLV</th>\n",
       "      <th>volatilityXLF</th>\n",
       "      <th>volatilityXLK</th>\n",
       "      <th>volatilityXTL</th>\n",
       "      <th>volatilityXLU</th>\n",
       "      <th>volatilityXLRE</th>\n",
       "      <th>volatilityTLRY</th>\n",
       "      <th>volatilityMU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>2019-06-10</td>\n",
       "      <td>0.262443</td>\n",
       "      <td>0.574566</td>\n",
       "      <td>0.333510</td>\n",
       "      <td>0.253769</td>\n",
       "      <td>0.281142</td>\n",
       "      <td>0.381002</td>\n",
       "      <td>0.409732</td>\n",
       "      <td>0.517354</td>\n",
       "      <td>0.406953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202950</td>\n",
       "      <td>0.150711</td>\n",
       "      <td>0.130785</td>\n",
       "      <td>0.183441</td>\n",
       "      <td>0.267121</td>\n",
       "      <td>0.202553</td>\n",
       "      <td>0.168077</td>\n",
       "      <td>0.155938</td>\n",
       "      <td>1.063379</td>\n",
       "      <td>0.412362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>2019-06-11</td>\n",
       "      <td>0.253383</td>\n",
       "      <td>0.573597</td>\n",
       "      <td>0.326130</td>\n",
       "      <td>0.252188</td>\n",
       "      <td>0.269323</td>\n",
       "      <td>0.369345</td>\n",
       "      <td>0.404458</td>\n",
       "      <td>0.531756</td>\n",
       "      <td>0.402374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187672</td>\n",
       "      <td>0.134942</td>\n",
       "      <td>0.109451</td>\n",
       "      <td>0.181765</td>\n",
       "      <td>0.260561</td>\n",
       "      <td>0.200543</td>\n",
       "      <td>0.150839</td>\n",
       "      <td>0.130113</td>\n",
       "      <td>1.092160</td>\n",
       "      <td>0.414424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2019-06-12</td>\n",
       "      <td>0.259465</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.331261</td>\n",
       "      <td>0.266931</td>\n",
       "      <td>0.278214</td>\n",
       "      <td>0.377129</td>\n",
       "      <td>0.405765</td>\n",
       "      <td>0.535325</td>\n",
       "      <td>0.401467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189846</td>\n",
       "      <td>0.136326</td>\n",
       "      <td>0.109338</td>\n",
       "      <td>0.192895</td>\n",
       "      <td>0.268465</td>\n",
       "      <td>0.203165</td>\n",
       "      <td>0.158621</td>\n",
       "      <td>0.129671</td>\n",
       "      <td>1.072741</td>\n",
       "      <td>0.527995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>0.231038</td>\n",
       "      <td>0.605956</td>\n",
       "      <td>0.304988</td>\n",
       "      <td>0.239902</td>\n",
       "      <td>0.232847</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.375983</td>\n",
       "      <td>0.518845</td>\n",
       "      <td>0.404909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156865</td>\n",
       "      <td>0.076984</td>\n",
       "      <td>0.091795</td>\n",
       "      <td>0.159182</td>\n",
       "      <td>0.234898</td>\n",
       "      <td>0.125690</td>\n",
       "      <td>0.158864</td>\n",
       "      <td>0.127678</td>\n",
       "      <td>1.070979</td>\n",
       "      <td>0.517080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>0.172617</td>\n",
       "      <td>0.533444</td>\n",
       "      <td>0.202580</td>\n",
       "      <td>0.234044</td>\n",
       "      <td>0.224527</td>\n",
       "      <td>0.340522</td>\n",
       "      <td>0.205248</td>\n",
       "      <td>0.225017</td>\n",
       "      <td>0.167734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131051</td>\n",
       "      <td>0.074474</td>\n",
       "      <td>0.099594</td>\n",
       "      <td>0.159437</td>\n",
       "      <td>0.207356</td>\n",
       "      <td>0.149072</td>\n",
       "      <td>0.159537</td>\n",
       "      <td>0.127758</td>\n",
       "      <td>0.946152</td>\n",
       "      <td>0.533473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Index  volatilityQQQ  volatilityTSLA  volatilityMSFT  \\\n",
       "863  2019-06-10       0.262443        0.574566        0.333510   \n",
       "864  2019-06-11       0.253383        0.573597        0.326130   \n",
       "865  2019-06-12       0.259465        0.628408        0.331261   \n",
       "866  2019-06-13       0.231038        0.605956        0.304988   \n",
       "867  2019-06-14       0.172617        0.533444        0.202580   \n",
       "\n",
       "     volatilityINTC  volatilityAAPL  volatilityNFLX  volatilityAMZN  \\\n",
       "863        0.253769        0.281142        0.381002        0.409732   \n",
       "864        0.252188        0.269323        0.369345        0.404458   \n",
       "865        0.266931        0.278214        0.377129        0.405765   \n",
       "866        0.239902        0.232847        0.353386        0.375983   \n",
       "867        0.234044        0.224527        0.340522        0.205248   \n",
       "\n",
       "     volatilityFB  volatilityGOOG      ...       volatilityXLY  volatilityXLP  \\\n",
       "863      0.517354        0.406953      ...            0.202950       0.150711   \n",
       "864      0.531756        0.402374      ...            0.187672       0.134942   \n",
       "865      0.535325        0.401467      ...            0.189846       0.136326   \n",
       "866      0.518845        0.404909      ...            0.156865       0.076984   \n",
       "867      0.225017        0.167734      ...            0.131051       0.074474   \n",
       "\n",
       "     volatilityXLV  volatilityXLF  volatilityXLK  volatilityXTL  \\\n",
       "863       0.130785       0.183441       0.267121       0.202553   \n",
       "864       0.109451       0.181765       0.260561       0.200543   \n",
       "865       0.109338       0.192895       0.268465       0.203165   \n",
       "866       0.091795       0.159182       0.234898       0.125690   \n",
       "867       0.099594       0.159437       0.207356       0.149072   \n",
       "\n",
       "     volatilityXLU  volatilityXLRE  volatilityTLRY  volatilityMU  \n",
       "863       0.168077        0.155938        1.063379      0.412362  \n",
       "864       0.150839        0.130113        1.092160      0.414424  \n",
       "865       0.158621        0.129671        1.072741      0.527995  \n",
       "866       0.158864        0.127678        1.070979      0.517080  \n",
       "867       0.159537        0.127758        0.946152      0.533473  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_data_orig = pd.read_csv('../data/files/multiple_concatenated_tickers_volatility.csv')\n",
    "vol_data_orig.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_orig = data_orig\\\n",
    "  .merge(vol_data_orig, how=\"inner\", left_on=data_orig.Index, right_on=vol_data_orig.Index).fillna(method=\"ffill\")\\\n",
    "  .drop([\"key_0\", \"Index_y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in list(data_orig.columns) if i == 'Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from keras.preprocessing import *\n",
    "from nltk.stem import SnowballStemmer\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "\n",
    "data = []\n",
    "for i in os.listdir(\"../briefings/\"):\n",
    "    #print(i)\n",
    "    f = open(\"../briefings/\"+str(i), \"rb\")\n",
    "    d = pickle.loads(f.read())\n",
    "    data.append(d)\n",
    "\n",
    "def categories(n, p):\n",
    "    n = float(n)\n",
    "    p = float(p)\n",
    "    if n >= p:\n",
    "        return [0, 0, 1]\n",
    "    if n <= -p:\n",
    "        return [1, 0, 0]\n",
    "    else:\n",
    "        return [0, 1, 0]\n",
    "    \n",
    "#Headlines\n",
    "headlines = []\n",
    "text_bodies = []\n",
    "all_text = []\n",
    "dates = []\n",
    "targets = []\n",
    "snow = SnowballStemmer('english')\n",
    "for i,v in enumerate(data):\n",
    "    for k in v:\n",
    "        if k[\"date\"] == -1:\n",
    "            continue\n",
    "        else:\n",
    "            ticker_data = k[\"tickerinfo\"]\n",
    "            if not ticker_data:\n",
    "                continue\n",
    "            if len(ticker_data[1]) == 4:\n",
    "                w = parse(k[\"date\"])\n",
    "                targets.append([w, float(ticker_data[1][3].replace(\"(\",\"\").replace(\")\",\"\").replace(\"%\",\"\"))]) # for regression\n",
    "                headlines.append(''.join([snow.stem(w) for w in str(k[\"headline\"])]))\n",
    "                text_bodies.append(''.join([snow.stem(w) for w in str(k[\"text\"])]))\n",
    "                all_text.append(''.join([snow.stem(w) for w in str(k[\"headline\"])]))\n",
    "                dates.append(w.date().isoformat())\n",
    "                all_text.append(''.join([snow.stem(w) for w in str(k[\"text\"])]))\n",
    "                dates.append(w.date().isoformat())\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=500, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets_df = pd.DataFrame(targets)\n",
    "targets_df[\"DATE\"] = [i[0].isoformat(sep=' ').split()[0] for i in targets]\n",
    "targets_df = targets_df.iloc[:,1:].groupby(['DATE'], as_index=True).mean()\n",
    "targets_df.columns = [\"pct_change\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_text)\n",
    "encoded_docs = tokenizer.texts_to_matrix(all_text, mode='count')\n",
    "tfidf_df = pd.DataFrame(encoded_docs)\n",
    "tfidf_df[\"DATE\"] = dates\n",
    "tfidf_docs = tfidf_df.groupby(['DATE'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>4.714286</td>\n",
       "      <td>2.178571</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.615385</td>\n",
       "      <td>1.923077</td>\n",
       "      <td>2.076923</td>\n",
       "      <td>1.807692</td>\n",
       "      <td>1.576923</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.961538</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.535714</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>1.821429</td>\n",
       "      <td>2.071429</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.892857</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.892857</td>\n",
       "      <td>2.392857</td>\n",
       "      <td>2.785714</td>\n",
       "      <td>2.821429</td>\n",
       "      <td>2.357143</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>2.178571</td>\n",
       "      <td>2.464286</td>\n",
       "      <td>1.857143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.714286</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>3.785714</td>\n",
       "      <td>1.928571</td>\n",
       "      <td>1.642857</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "DATE                                                                          \n",
       "2015-12-31  0.0  9.250000  4.714286  2.178571  2.571429  2.571429  2.000000   \n",
       "2016-01-04  0.0  7.615385  1.923077  2.076923  1.807692  1.576923  2.000000   \n",
       "2016-01-05  0.0  6.535714  4.857143  1.821429  2.071429  1.214286  1.285714   \n",
       "2016-01-06  0.0  8.892857  2.392857  2.785714  2.821429  2.357143  1.928571   \n",
       "2016-01-07  0.0  7.714286  2.428571  3.785714  1.928571  1.642857  1.750000   \n",
       "\n",
       "                 7         8         9      ...          490       491  \\\n",
       "DATE                                        ...                          \n",
       "2015-12-31  2.000000  2.285714  1.571429    ...     0.000000  0.035714   \n",
       "2016-01-04  1.961538  4.000000  1.000000    ...     0.000000  0.000000   \n",
       "2016-01-05  1.892857  1.857143  0.642857    ...     0.035714  0.000000   \n",
       "2016-01-06  2.178571  2.464286  1.857143    ...     0.000000  0.000000   \n",
       "2016-01-07  2.571429  3.071429  0.964286    ...     0.000000  0.035714   \n",
       "\n",
       "                 492       493       494       495       496  497       498  \\\n",
       "DATE                                                                          \n",
       "2015-12-31  0.035714  0.000000  0.035714  0.000000  0.000000  0.0  0.000000   \n",
       "2016-01-04  0.038462  0.076923  0.038462  0.000000  0.000000  0.0  0.115385   \n",
       "2016-01-05  0.035714  0.000000  0.000000  0.035714  0.000000  0.0  0.000000   \n",
       "2016-01-06  0.071429  0.071429  0.035714  0.035714  0.035714  0.0  0.071429   \n",
       "2016-01-07  0.107143  0.071429  0.035714  0.000000  0.000000  0.0  0.000000   \n",
       "\n",
       "                 499  \n",
       "DATE                  \n",
       "2015-12-31  0.035714  \n",
       "2016-01-04  0.038462  \n",
       "2016-01-05  0.035714  \n",
       "2016-01-06  0.035714  \n",
       "2016-01-07  0.035714  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(859, 1, 5000)\n",
      "(859, 1)\n",
      "x_train (687, 1, 5000)\n",
      "y_train (687, 1)\n",
      "x_test (172, 1, 5000)\n",
      "y_test (172, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X = tfidf_docs.as_matrix()\n",
    "# y = targets_df.as_matrix() # predict current\n",
    "y = targets_df.shift(-1).dropna().as_matrix() # predict next day\n",
    "\n",
    "X = np.array([[i] for i in X]) # for batch size 1\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(batch_input_shape=(1, 1, 500..., return_sequences=True, stateful=True, units=300)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation time :  0.031242847442626953\n"
     ]
    }
   ],
   "source": [
    "LAYERS = 300\n",
    "model = Sequential()\n",
    "\n",
    "# Original Architecture\n",
    "# model.add(Dense(\n",
    "#     input_dim=x_train.shape[1],\n",
    "#     output_dim=LAYERS))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(\n",
    "#     input_dim=x_train.shape[1],\n",
    "#     output_dim=LAYERS))\n",
    "# #model.add(Dense(output_dim = y_train.shape[1]))\n",
    "# model.add(Dense(output_dim=y_train.shape[1]))\n",
    "# model.add(Activation('linear'))\n",
    "\n",
    "# Stateful LSTM Architecture\n",
    "model.add(LSTM(\n",
    "    batch_input_shape=(1, 1, 5000),\n",
    "    output_dim=LAYERS,\n",
    "    return_sequences=True,\n",
    "    stateful=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(\n",
    "    LAYERS,\n",
    "    return_sequences=False,\n",
    "    stateful=True))\n",
    "model.add(Dropout(0.5))\n",
    "# model.add(Dense(output_dim=y_train.shape[1]))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "start = time.time()\n",
    "adam = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='mse', optimizer=adam)\n",
    "print('compilation time : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 618 samples, validate on 69 samples\n",
      "Epoch 1/5000\n",
      "618/618 [==============================] - 97s 157ms/step - loss: 71.0963 - val_loss: 0.7959\n",
      "Epoch 2/5000\n",
      "618/618 [==============================] - 82s 132ms/step - loss: 64.5788 - val_loss: 0.6346\n",
      "Epoch 3/5000\n",
      "618/618 [==============================] - 84s 136ms/step - loss: 77.0483 - val_loss: 1.9756\n",
      "Epoch 4/5000\n",
      "618/618 [==============================] - 84s 136ms/step - loss: 71.4015 - val_loss: 0.4628\n",
      "Epoch 5/5000\n",
      "618/618 [==============================] - 88s 142ms/step - loss: 68.9911 - val_loss: 0.9938\n",
      "Epoch 6/5000\n",
      "618/618 [==============================] - 94s 152ms/step - loss: 64.7073 - val_loss: 1.9643\n",
      "Epoch 7/5000\n",
      "618/618 [==============================] - 90s 146ms/step - loss: 71.7630 - val_loss: 11.6875\n",
      "Epoch 8/5000\n",
      "618/618 [==============================] - 90s 146ms/step - loss: 68.5788 - val_loss: 2.0517\n",
      "Epoch 9/5000\n",
      "618/618 [==============================] - 92s 148ms/step - loss: 68.9574 - val_loss: 4.9945\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.05000000074505806.\n",
      "Epoch 10/5000\n",
      "618/618 [==============================] - 91s 148ms/step - loss: 45.2450 - val_loss: 0.3988\n",
      "Epoch 11/5000\n",
      "618/618 [==============================] - 101s 163ms/step - loss: 42.3790 - val_loss: 0.5171\n",
      "Epoch 12/5000\n",
      "618/618 [==============================] - 96s 155ms/step - loss: 45.0893 - val_loss: 0.9836\n",
      "Epoch 13/5000\n",
      "618/618 [==============================] - 100s 162ms/step - loss: 43.9009 - val_loss: 1.7265\n",
      "Epoch 14/5000\n",
      "618/618 [==============================] - 101s 163ms/step - loss: 45.1990 - val_loss: 2.4596\n",
      "Epoch 15/5000\n",
      "618/618 [==============================] - 90s 145ms/step - loss: 43.8446 - val_loss: 0.5562\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.02500000037252903.\n",
      "Epoch 16/5000\n",
      "618/618 [==============================] - 90s 146ms/step - loss: 34.2903 - val_loss: 0.4466\n"
     ]
    }
   ],
   "source": [
    "MODELNAME = 'briefing_nextday_stateful_lstm'\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=6, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('./models/'+MODELNAME+'_best.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, epsilon=1e-4, mode='min')\n",
    "tbrd = TensorBoard(log_dir=\"./logs\", histogram_freq=0, batch_size=128, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "max_len = 20\n",
    "class ResetStatesCallback(Callback):\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        if self.counter % max_len == 0:\n",
    "            self.model.reset_states()\n",
    "        self.counter += 1\n",
    "\n",
    "VALIDATIONSIZE = 0.10\n",
    "EPOCHS = 5000\n",
    "# model = keras.models.load_model('./models/'+MODELNAME+'_best.hdf5') \n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=1,\n",
    "    nb_epoch=EPOCHS,\n",
    "    validation_split=VALIDATIONSIZE,\n",
    "    callbacks = [reduce_lr_loss, earlyStopping, mcp_save, tbrd, ResetStatesCallback()],\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write callback that looks for loss below threshold and that loss and val_loss are less than 0.01 difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 2s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9353546145132183"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate(x_test, y_test)\n",
    "model.evaluate(x_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 3s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.884816817182455"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model('./models/'+MODELNAME+'_best.hdf5') \n",
    "# model.evaluate(x_test, y_test)\n",
    "model.evaluate(x_test, y_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14092773]]\n",
      "[[0.14935726]]\n",
      " \n",
      "[[0.10491294]]\n",
      "[[0.08785099]]\n",
      "[[0.08546847]]\n",
      " \n",
      "[[0.0851447]]\n",
      "[[0.16033787]]\n",
      "[[0.6232299]]\n",
      "[[0.2937979]]\n",
      "[[0.16033787]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# print(model.predict(tfidf_docs.loc[\"2016-01-06\"].as_matrix().reshape(1,-1))) # known down day\n",
    "# print(model.predict(tfidf_docs.loc[\"2016-03-09\"].as_matrix().reshape(1,-1))) # known up day\n",
    "print(model.predict(np.array([tfidf_docs.loc[\"2016-01-06\"].as_matrix().reshape(1,-1)])))\n",
    "print(model.predict(np.array([tfidf_docs.loc[\"2016-03-09\"].as_matrix().reshape(1,-1)])))\n",
    "\n",
    "def predict_based_on_text(text):\n",
    "#     return model.predict(tokenizer.texts_to_matrix([text]).reshape(1,-1))\n",
    "    return model.predict(np.array([tokenizer.texts_to_matrix([text]).reshape(1,-1)]))\n",
    "\n",
    "print(\" \")\n",
    "print(predict_based_on_text(\"U.S. dollar index weakens\"))\n",
    "print(predict_based_on_text(\"trade talks begin\"))\n",
    "print(predict_based_on_text(\"crude oil miss\"))\n",
    "\n",
    "# 6/14\n",
    "print(\" \")\n",
    "print(predict_based_on_text(\"Large Cap Underperformance Weighs On Dow\"))\n",
    "print(predict_based_on_text(\"Wall Street slips on semiconductor weakness\"))\n",
    "print(predict_based_on_text(\"FAANG stocks trade mixed\"))\n",
    "print(predict_based_on_text(\"Amazon underpinning resiliency in stock market this week\"))\n",
    "print(predict_based_on_text(\"Facebook gains on crypto plans\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
